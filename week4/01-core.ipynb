{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3b4454e-83c8-41b5-a4d8-dc5c5a010302",
   "metadata": {},
   "source": [
    "# Level 1: Query Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c1bfc0c-9295-40d9-b047-d1da2c4b32c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.foundation import L\n",
    "from fastcore.basics import *\n",
    "from fastcore.test import *\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e50a62c7-6cdb-49e9-a2a4-64f75ecc5d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a61fffda-2060-4389-a21d-52f4d9dc6391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "_re_spec = re.compile(r'([/#\\\\-\\\\.:\\'\\\"])')\n",
    "\n",
    "def spec_add_spaces(t):\n",
    "    \"Add spaces around \\\" ' . : - / \\ and #\"\n",
    "    return _re_spec.sub(r' \\1 ', t)\n",
    "\n",
    "# Causes the resulting RE to match from m to n repetitions of the preceding RE, attempting to match as many repetitions as possible.\n",
    "_re_space = re.compile(' {2,}')\n",
    "\n",
    "def rm_useless_spaces(t):\n",
    "    \"Remove multiple spaces\"\n",
    "    return _re_space.sub(' ', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f8aadb4-cd61-41e7-8e19-e7ce2ebaa979",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(spec_add_spaces(\".nltk\"), ' . nltk')\n",
    "test_eq(spec_add_spaces(\"nltk:\"), 'nltk : ')\n",
    "test_eq(spec_add_spaces(\"nltk'\"), \"nltk ' \")\n",
    "test_eq(spec_add_spaces(\"nltk\\\"\"), 'nltk \" ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf424ff-575c-436c-b869-54338583c425",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prune the Category Taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a36c40f-a4c3-44e8-8062-5caa1570853b",
   "metadata": {},
   "source": [
    "## Transform Queries\n",
    "\n",
    "Convert the queries to lowercase, strip quotation marks (and perhaps other punctuation), and optionally implement other normalization, like using the nltk stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6d36948-d158-429c-9b30-2d4fdb1c8dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_punct(t):\n",
    "    for p in string.punctuation:\n",
    "        t = t.replace(p, ' ')\n",
    "    return t\n",
    "\n",
    "def transform(query):\n",
    "    \"Transform query by replacing punctuations, removing multiple spaces, stemming, lower casing\"\n",
    "    \n",
    "    # replace_punct\n",
    "    query = spec_add_spaces(query)\n",
    "    \n",
    "    # remove punct\n",
    "    query = rm_punct(query)\n",
    "    \n",
    "    # replace multiple spaces\n",
    "    query = rm_useless_spaces(query)\n",
    "    \n",
    "    # fix registered, trademark, copyright symbol\n",
    "    # remove non-ascii characters from query\n",
    "    query = query.encode(encoding='ascii', errors='ignore').decode()\n",
    "    \n",
    "    query = ' '.join([o for o in query.split(' ') if not o.isnumeric()])\n",
    "    \n",
    "    # add stemmer\n",
    "    query = stemmer.stem(query)\n",
    "    return query.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bee1f2f-9b77-4fd2-a057-07417f24bfa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# Useful if you want to perform stemming.\n",
    "import nltk\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "categories_file_name = r'/workspace/datasets/product_data/categories/categories_0001_abcat0010000_to_pcmcat99300050000.xml'\n",
    "\n",
    "queries_file_name = r'/workspace/datasets/train.csv'\n",
    "output_file_name = r'/workspace/datasets/labeled_query_data.txt'\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='Process arguments.')\n",
    "# general = parser.add_argument_group(\"general\")\n",
    "# general.add_argument(\"--min_queries\", default=1,  help=\"The minimum number of queries per category label (default is 1)\")\n",
    "# general.add_argument(\"--output\", default=output_file_name, help=\"the file to output to\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# output_file_name = args.output\n",
    "\n",
    "# if args.min_queries:\n",
    "#     min_queries = int(args.min_queries)\n",
    "# The root category, named Best Buy with id cat00000, doesn't have a parent.\n",
    "root_category_id = 'cat00000'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af32d450-2664-4a45-862e-1b18f44d7b60",
   "metadata": {},
   "source": [
    "Read the category tree from the categories XML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "249fa89b-0906-4484-962d-8b639cda9ba6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tree = ET.parse(categories_file_name)\n",
    "root = tree.getroot()\n",
    "\n",
    "# Parse the category XML file to map each category id to its parent category id in a dataframe.\n",
    "categories = []\n",
    "parents = []\n",
    "for child in root:\n",
    "    id = child.find('id').text\n",
    "    cat_path = child.find('path')\n",
    "    cat_path_ids = [cat.find('id').text for cat in cat_path]\n",
    "    leaf_id = cat_path_ids[-1]\n",
    "    if leaf_id != root_category_id:\n",
    "        categories.append(leaf_id)\n",
    "        parents.append(cat_path_ids[-2])\n",
    "parents_df = pd.DataFrame(list(zip(categories, parents)), columns =['category', 'parent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dc41c0a-a31c-481e-bc2f-e07c043e6ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training data into pandas, only keeping queries with non-root categories in our category tree.\n",
    "df = pd.read_csv(queries_file_name)[['category', 'query']]\n",
    "df = df[df['category'].isin(categories)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b2e3d41-418f-4a13-8cbf-a3eafb94aac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENTED: Convert queries to lowercase, and optionally implement other normalization, like stemming.\n",
    "df['query'] = df['query'].apply(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3311b311-875f-49d9-92ec-8ecb9923e556",
   "metadata": {},
   "source": [
    "## Query Count of Leaf Categories\n",
    "\n",
    "Compute the query count of all leaf categories.\n",
    "\n",
    "What is leaf? \n",
    "A category that is not a parent. So it cannot exist in the parent column. For each category check if exists in parent column, then update is_leaf value for that row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b87de8c3-eeb6-47c5-aa0e-d1e973299c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(len(parents_df), 4639)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34ede4d8-4f0c-43cd-9c6e-fb20f80b878d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(root_category_id in parents_df.values, True)\n",
    "test_eq(root_category_id in categories, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5c97a5c-4000-49c0-9749-b86ff56f3deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "parents_df['is_leaf'] = ~parents_df.category.isin(parents_df.parent)\n",
    "parents_df['roll_up'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "797b22c0-f974-4ab1-876f-cdeb8d67b883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parent(cat):\n",
    "    if cat in parents_df.category.values:\n",
    "        return parents_df[parents_df.category == cat ].parent.to_list()[0]\n",
    "    else:\n",
    "        return root_category_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "224d430c-1c77-4d2f-8c33-0bc6af4af4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "aleaf = parents_df[parents_df.is_leaf].category.values[0] # 'abcat0011001'\n",
    "test_eq(len(parents_df[parents_df.parent == aleaf]), 0)\n",
    "\n",
    "expected = 'abcat0011000'\n",
    "test_eq(parent('abcat0011003'), expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f1420f7-4240-4045-b3ee-d672bfbba294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accepts a seq of tuple\n",
    "data = L(df.category.value_counts().to_dict().items())\n",
    "qcount = pd.DataFrame.from_records(data, columns=['cat', 'count'])\n",
    "\n",
    "cat_counts = df.category.value_counts().to_dict()\n",
    "\n",
    "# Compute the query counts of all categories from train.csv and 0 if category not present\n",
    "parent_counts = [cat_counts.get(o['category'], 0) for _, o in parents_df.iterrows()]\n",
    "parents_df['counts'] = parent_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66f1f30-0604-4e74-af8f-2fd34b79b033",
   "metadata": {},
   "source": [
    "## Pruning: Rollup Categories to Min. # of Queries per Category\n",
    "\n",
    "For example, if the minimum number of queries is 100 and there are only 99 queries mapped to “Best Buy > Musical Instruments > Guitars > Bass Guitars”, then those queries would be mapped to parent category “Best Buy > Musical Instruments > Guitars”. If the minimum number of queries were higher (e.g., 1,000), then those queries might have to be rolled up to an even broader category, like ““Best Buy > Musical Instruments”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e2d7633-8221-4c0e-871d-b049574e07b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bacea28f-f728-48aa-821b-96fbd0bad1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollup(df, min_queries=1000):\n",
    "    print(f\"Min # of Queries per Category : {min_queries}\")\n",
    "    cat_count = df.groupby('category').size().to_frame('count')\n",
    "\n",
    "    print(f\"# of Categories before pruning : {len(df.category.value_counts())}\")\n",
    "\n",
    "    # categories to be pruned\n",
    "    prune_df = cat_count[cat_count['count'] < min_queries];\n",
    "\n",
    "    while len(prune_df) > 0:\n",
    "        for cat, count in prune_df.iterrows():\n",
    "            df.replace(to_replace=cat, value=parent(cat), inplace=True)\n",
    "        cat_count = df.groupby('category').size().to_frame('count')\n",
    "        prune_df = cat_count[cat_count['count'] < min_queries]\n",
    "        print(f\"Categories to be pruned : {len(prune_df)}\")\n",
    "    \n",
    "    print()\n",
    "    print(f\"# of Categories after pruning : {len(df.category.value_counts())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0e88b3c-21c7-41fb-a9ea-8342372dd386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min # of Queries per Category : 100\n",
      "# of Categories before pruning : 1486\n",
      "Categories to be pruned : 140\n",
      "Categories to be pruned : 33\n",
      "Categories to be pruned : 5\n",
      "Categories to be pruned : 2\n",
      "Categories to be pruned : 1\n",
      "Categories to be pruned : 0\n",
      "\n",
      "# of Categories after pruning : 878\n"
     ]
    }
   ],
   "source": [
    "df = source\n",
    "rollup(df, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82dba788-4486-4e59-bfe6-cfde77b54e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_queries=1000\n",
    "df = source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86b6fcce-fa30-400f-bf65-8c8faaa6bed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Categories before pruning : 1486\n",
      "Categories to be pruned : 232\n",
      "Categories to be pruned : 42\n",
      "Categories to be pruned : 5\n",
      "Categories to be pruned : 1\n",
      "Categories to be pruned : 0\n",
      "\n",
      "# of Categories after pruning : 386\n"
     ]
    }
   ],
   "source": [
    "# IMPLEMENTED: Roll up categories to ancestors to satisfy the minimum number of queries per category.\n",
    "cat_count = df.groupby('category').size().to_frame('count')\n",
    "\n",
    "print(f\"# of Categories before pruning : {len(df.category.value_counts())}\")\n",
    "\n",
    "# categories to be pruned\n",
    "prune_df = cat_count[cat_count['count'] < min_queries];\n",
    "\n",
    "while len(prune_df) > 0:\n",
    "    for cat, count in prune_df.iterrows():\n",
    "        df.replace(to_replace=cat, value=parent(cat), inplace=True)\n",
    "    cat_count = df.groupby('category').size().to_frame('count')\n",
    "    prune_df = cat_count[cat_count['count'] < min_queries]\n",
    "    print(f\"Categories to be pruned : {len(prune_df)}\")\n",
    "    \n",
    "print()\n",
    "print(f\"# of Categories after pruning : {len(df.category.value_counts())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80d34860-e2e6-47bd-aa3e-2fd16a7f67a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels in fastText format.\n",
    "df['label'] = '__label__' + df['category']\n",
    "\n",
    "# Output labeled query data as a space-separated file, making sure that every category is in the taxonomy.\n",
    "df = df[df['category'].isin(categories)]\n",
    "df['output'] = df['label'] + ' ' + df['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c7852e44-3a11-4bad-8266-ae7acec076f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['output']].to_csv(output_file_name, header=False, sep='|', escapechar='\\\\', quoting=csv.QUOTE_NONE, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069e9c74-7939-4bf5-9f2b-de4e8f72065a",
   "metadata": {},
   "source": [
    "# Train a Classifier (with min Q set to 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "236ad984-cbec-4081-ac41-cd7772ca8a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_res(res, k=1): \n",
    "    print(f\"N\\t: {res[0]} \\nP@{k}\\t: {res[1]:.3f} \\nR@{k}\\t: {res[2]:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "597d1f16-747c-40a4-ba21-94b61b6eb90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/29576430/shuffle-dataframe-rows\n",
    "#df = df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcb53b44-8f8b-45b3-abb5-87c8bdeb6437",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuf_output_fname = '/workspace/datasets/shuf_labeled_query_data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1b58af9-25d3-4bca-a869-26cdcac96c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '/workspace/datasets/queries.train'\n",
    "test_file = '/workspace/datasets/queries.test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9aba1ab-39e3-4204-b38a-5bef9425de2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/workspace/datasets/labeled_query_data.txt',\n",
       " '/workspace/datasets/shuf_labeled_query_data.txt')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_file_name, shuf_output_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1462235-672f-477e-a7a9-0bbe21d41578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the input before train/test split\n",
    "!shuf {output_file_name} > {shuf_output_fname}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783a21ed-1c72-420d-8f3e-ad090bbc8385",
   "metadata": {},
   "source": [
    "Set aside first 50000 rows as train and last 10000 rows as test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "383a537a-56a2-4e4b-bbff-2348710dac0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 50000 {shuf_output_fname} > {train_file}\n",
    "!tail -n 50000 {shuf_output_fname} > {test_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544fad65-d8a7-491f-9f67-a7bf7b1b7cd8",
   "metadata": {},
   "source": [
    "There may be a problem with the way we are splitting here. Some of the categories in train may not be in test and vice versa. Ideally we should have used stratified train test split to ensure the category distribution is similar in train & test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca2cbf2d-0417-4b16-8a16-d000c67b09f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  7793\n",
      "Number of labels: 385\n",
      "Progress: 100.0% words/sec/thread:   13485 lr:  0.000000 avg.loss:  4.263705 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model=ft.train_supervised(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7b96667-ff04-4077-b72a-d33d011766b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#385) ['__label__cat02015','__label__abcat0101001','__label__pcmcat247400050000','__label__pcmcat209000050008','__label__pcmcat144700050004','__label__pcmcat209400050001','__label__abcat0703002','__label__pcmcat247400050001','__label__abcat0201011','__label__pcmcat209000050007'...]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L(model.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ea6432-252a-4f97-b93f-c5dc13c30a07",
   "metadata": {},
   "source": [
    "## Fasttext with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a6eacce-d4b2-4a5c-be57-8f3310eb947f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 0.1, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.epoch, model.lr, model.wordNgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1661352-c47b-4bd4-a4d3-0f04b8adf4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t: 50000 \n",
      "P@1\t: 0.470 \n",
      "R@1\t: 0.470\n",
      "\n",
      "\n",
      "N\t: 50000 \n",
      "P@3\t: 0.212 \n",
      "R@3\t: 0.636\n",
      "\n",
      "\n",
      "N\t: 50000 \n",
      "P@5\t: 0.139 \n",
      "R@5\t: 0.694\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = model.test(test_file, k=1);print_res(res, k=1); print()\n",
    "res = model.test(test_file, k=3);print_res(res, k=3); print()\n",
    "res = model.test(test_file, k=5);print_res(res, k=5); print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e969d4-1a1d-410d-ac44-7110566eed5a",
   "metadata": {},
   "source": [
    "## Fasttext with optimized parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e637b536-c3f6-4d4d-bbf0-3dde178a5953",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  7793\n",
      "Number of labels: 385\n",
      "Progress: 100.0% words/sec/thread:   13571 lr:  0.000000 avg.loss:  2.055373 ETA:   0h 0m 0s 78.3% words/sec/thread:   13697 lr:  0.043406 avg.loss:  2.285883 ETA:   0h 0m 5sm 2s100.0% words/sec/thread:   13571 lr: -0.000005 avg.loss:  2.055373 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model = ft.train_supervised(train_file, epoch=25, lr=0.2, wordNgrams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1d4d8a13-65f4-4dfc-afb1-d946df0794ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t: 50000 \n",
      "P@1\t: 0.519 \n",
      "R@1\t: 0.519\n",
      "\n",
      "\n",
      "N\t: 50000 \n",
      "P@3\t: 0.235 \n",
      "R@3\t: 0.705\n",
      "\n",
      "\n",
      "N\t: 50000 \n",
      "P@5\t: 0.154 \n",
      "R@5\t: 0.770\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = model.test(test_file, k=1);print_res(res, k=1); print()\n",
    "res = model.test(test_file, k=3);print_res(res, k=3); print()\n",
    "res = model.test(test_file, k=5);print_res(res, k=5); print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a4d74202-f2ba-48f8-944c-431912750489",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path='/workspace/datasets/fasttext'\n",
    "model.save_model(f'{model_path}/query_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d74e024-95a6-41fd-9d9a-284e88288008",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path='/workspace/datasets/fasttext'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ed4da15-1458-48e9-980e-f42b22e78e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "query_model = ft.load_model(f'{model_path}/query_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e80cbb7-c328-43b9-b8bd-3f6e51dd0a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__pcmcat139900050002', '__label__abcat0202003', '__label__cat09000'),\n",
       " array([0.2879619 , 0.16255598, 0.09963983]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_model.predict('satelite radio', k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0783a299-67b6-4be0-a3dd-99a374c0d7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats, conf = query_model.predict('blue ray dvr', k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e33c1ea6-e574-4ad6-acfc-dcce7163407c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__abcat0102003',\n",
       "  '__label__abcat0515004',\n",
       "  '__label__pcmcat205900050012'),\n",
       " array([0.46350497, 0.03990539, 0.03567554]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats, conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "814f4f4e-51a0-49e6-8a87-88f01d13b2bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__pcmcat144700050004', '__label__abcat0208011', '__label__cat09000'),\n",
       " array([0.76492441, 0.14439672, 0.03154249]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_model.predict('bose headphon', k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5527fc18-faa0-438d-a3e5-c1cc9f939f64",
   "metadata": {},
   "source": [
    "## Train a classifier with min # of queries set to 100 (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ff79476-4257-4c20-95d8-181a4d0dd306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  7738\n",
      "Number of labels: 874\n",
      "Progress: 100.0% words/sec/thread:    6539 lr:  0.000000 avg.loss:  5.382691 ETA:   0h 0m 0s ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t: 49987 \n",
      "P@1\t: 0.461 \n",
      "R@1\t: 0.461\n",
      "\n",
      "\n",
      "N\t: 49987 \n",
      "P@3\t: 0.205 \n",
      "R@3\t: 0.614\n",
      "\n",
      "\n",
      "N\t: 49987 \n",
      "P@5\t: 0.135 \n",
      "R@5\t: 0.676\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fasttext as ft\n",
    "\n",
    "shuf_output_fname = '/workspace/datasets/shuf_labeled_query_data_100.txt'\n",
    "output_file_name = '/workspace/datasets/labeled_query_data_100.txt'\n",
    "\n",
    "train_file = '/workspace/datasets/queries.train'\n",
    "test_file = '/workspace/datasets/queries.test'\n",
    "\n",
    "# Shuffle the input before train/test split\n",
    "!shuf {output_file_name} > {shuf_output_fname}\n",
    "\n",
    "# Set aside first 50000 rows as train and last 10000 rows as test\n",
    "\n",
    "!head -n 50000 {shuf_output_fname} > {train_file}\n",
    "!tail -n 50000 {shuf_output_fname} > {test_file}\n",
    "\n",
    "# There may be a problem with the way we are splitting here. Some of the categories in train may not be in test and vice versa. Ideally we should have used stratified train test split to ensure the category distribution is similar in train & test dataset.\n",
    "\n",
    "model=ft.train_supervised(train_file)\n",
    "\n",
    "res = model.test(test_file, k=1);print_res(res, k=1); print()\n",
    "res = model.test(test_file, k=3);print_res(res, k=3); print()\n",
    "res = model.test(test_file, k=5);print_res(res, k=5); print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbe8bb8f-f314-4f16-8ed9-8396f5999fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train a classifier with min # of queries set to 100 (optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80a7f02f-6db5-40b0-a18e-7ae97ed0f475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  7727\n",
      "Number of labels: 871\n",
      "Progress:  99.9% words/sec/thread:    6488 lr:  0.000107 avg.loss:  2.313887 ETA:   0h 0m 0s 59.8% words/sec/thread:    6491 lr:  0.080374 avg.loss:  2.917681 ETA:   0h 0m19s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t: 49978 \n",
      "P@1\t: 0.511 \n",
      "R@1\t: 0.511\n",
      "\n",
      "\n",
      "N\t: 49978 \n",
      "P@3\t: 0.232 \n",
      "R@3\t: 0.695\n",
      "\n",
      "\n",
      "N\t: 49978 \n",
      "P@5\t: 0.151 \n",
      "R@5\t: 0.755\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% words/sec/thread:    6478 lr:  0.000000 avg.loss:  2.313320 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "import fasttext as ft\n",
    "\n",
    "shuf_output_fname = '/workspace/datasets/shuf_labeled_query_data_100.txt'\n",
    "output_file_name = '/workspace/datasets/labeled_query_data_100.txt'\n",
    "\n",
    "train_file = '/workspace/datasets/queries.train'\n",
    "test_file = '/workspace/datasets/queries.test'\n",
    "\n",
    "# Shuffle the input before train/test split\n",
    "!shuf {output_file_name} > {shuf_output_fname}\n",
    "\n",
    "# Set aside first 50000 rows as train and last 10000 rows as test\n",
    "\n",
    "!head -n 50000 {shuf_output_fname} > {train_file}\n",
    "!tail -n 50000 {shuf_output_fname} > {test_file}\n",
    "\n",
    "# There may be a problem with the way we are splitting here. Some of the categories in train may not be in test and vice versa. Ideally we should have used stratified train test split to ensure the category distribution is similar in train & test dataset.\n",
    "\n",
    "model=ft.train_supervised(train_file, epoch=25, lr=0.2, wordNgrams=2)\n",
    "\n",
    "res = model.test(test_file, k=1);print_res(res, k=1); print()\n",
    "res = model.test(test_file, k=3);print_res(res, k=3); print()\n",
    "res = model.test(test_file, k=5);print_res(res, k=5); print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c5904d-5c0d-4877-910a-6a2c785d9a5d",
   "metadata": {},
   "source": [
    "# Manual Query Classificaition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e0a42355-19b8-4c09-9481-2b867ed82106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__cat09000 satelite radio\n",
      "__label__abcat0901000 wine cool\n",
      "__label__pcmcat144700050004 drake beat\n",
      "__label__cat02607 music\n",
      "__label__abcat0208024 chauvet\n",
      "__label__abcat0106016 elit\n",
      "__label__abcat0102007 blue ray dvr\n",
      "__label__abcat0410010 canon camera flash\n",
      "__label__cat02015 lion k\n",
      "__label__pcmcat144700050004 bose headphon\n"
     ]
    }
   ],
   "source": [
    "! head {test_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d81313c2-4ec9-4b34-aada-60acf33d6be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__abcat0201010 waterproof\n",
      "__label__cat02015 lion k\n",
      "__label__pcmcat248700050021 radio\n",
      "__label__pcmcat183800050007 labtop pow\n",
      "__label__pcmcat158900050018 projector\n",
      "__label__pcmcat128500050004 speaker stand\n",
      "__label__pcmcat209000050008 toshiba thr\n",
      "__label__cat02015 thor\n",
      "__label__abcat0101001 vizio\n",
      "__label__cat02004 turntabl\n"
     ]
    }
   ],
   "source": [
    "!tail {train_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ac989b3-f2db-4cda-9072-d982a6e6f2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python ../opensearch/categoryViewer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "267568fa-a70e-4521-a873-38e3d1a4709e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat09000'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' __label__cat09000 '.strip()[len('__label__'):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bae5d9b-8720-4faf-ab9e-10df4469f0b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sml_week4",
   "language": "python",
   "name": "sml_week4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
